LunarLander-v3:
  # Training
  episodes: 600
  max_episode_steps: 1000
  eval_episodes: 10
  eval_every: 25

  # TD3 core hyperparameters
  learning_rate: 0.0001 # Adam LR for actor & critics
  discount_factor: 0.99 # γ
  tau: 0.005 # target soft update rate
  policy_noise: 0.1
  noise_clip: 0.2
  policy_delay: 2 # delayed policy update (every 2 critic steps)
  exploration_noise: 0.03 # σ for exploration noise added to actions

  # Replay buffer
  batch_size: 256
  replay_memory_size: 1000000
  warmup_steps: 5000 # collect random experience before learning

CarRacing-v3:
  # Training
  episodes: 2000
  max_episode_steps: 1000
  eval_every: 50
  eval_episodes: 5

  # TD3 core hyperparameters
  learning_rate: 0.0001 # smaller is more stable in image tasks
  discount_factor: 0.99
  tau: 0.005
  policy_noise: 0.2
  noise_clip: 0.3 # slightly lower → smoother actions
  policy_delay: 2
  exploration_noise: 0.1 # keep but may reduce later to 0.05

  # Replay buffer
  batch_size: 256
  replay_memory_size: 1000000
  warmup_steps: 30000 # must be large for CNN-based state

